# Mini Project 1 - CMSC818B Decisioin Making for Robotics

## ICRA 2023 Workshop - ScalableAD
_This workshop presents good literature related to solving challenging problems in Autonomous Driving Systems_
<br>
[ScalableAD website](https://sites.google.com/view/icra2023av)

The ICRA workshop on scalable autonomous driving addresses some of the biggest and complex challenges that are being faced while developing autonomous vehicles. The issue of scalability has been considered from various viewpoints in robotics in this workshop. The viewpoints span from issues in generating rare driving scenarios for training autonomous agents, developing new perception techniques that can fuse multiple sensor information, to connected autonomous vehicles (AVs).

The scalability issue of AVs  is of utmost importance if we wish to deploy them at scale in multiple cities across the globe. As we find more efficient methods for solving different problems faced by AVs, such as generating BEV (Bird’s Eye View) by fusing multiple on-board sensors, our ability to test and enhance such methods should also improve. This is only possible if we consciously push towards more scalable algorithms and techniques that are designed for AVs. By having scalable technologies, we can ensure that the solutions devised are globally optimum and generalize well across different scenarios that the autonomous agent may face while driving.

### Index:
1. [F2BEV: Bird's Eye View Generation from Surround-View Fisheye Camera Images](#topic1)
2. [Scaling AV through Adversarial Scenario Generation and Continual Learning](#topic2)
3. [A V2V-based Collaborative Navigation Framework for Connected Autonomous Vehicles in Occluded Scenarios](#topic3)
4. [References](#refs)


Demo equation: 
<br/>
![formula](https://render.githubusercontent.com/render/math?math=e^{i \pi} = -1)
<br/>
![formula](https://render.githubusercontent.com/render/math?math= \sqrt{3x-1}+(1+x)^2)

### <a name="topic1"></a>Topic 1: F2BEV: Bird's Eye View Generation from Surround-View Fisheye Camera Images for Automated Driving.[1](#ref1)

In [1](#ref1), Samani, et. al. propose a baseline algorithm to generate BEV (Bird’s Eye View) representation constructed from surround-view fisheye cameras. As more and more AVs (Autonomous Vehicles) are deployed, the higher dependency on vision based sensors and ranging sensors is bound to grow exponentially. This higher demand may lead to bottlenecks in sensor supply chains, causing deployment delays and increasing cost of AVs for the users. Using fisheye cameras, fewer cameras are needed to capture the entire surroundings of an AV. Thus, by reducing the number of cameras, hardware requirements and possible points of failure are reduced.

The paper has 2 major contributions in the area of generating BEV. Firstly, they propose the baseline model F2BEV that generates BEV from Fisheye cameras. This method is the first algorithm that works on creating BEV using distortion aware models. The existing models work by undistorting the images from fisheye cameras and then proceeding with traditional methods for BEV generation. By using this novel approach, the authors outperform not only the approaches for BEV generation from fisheye cameras, but also BEV generation from traditional pinhole cameras. The authors also consider single-task and multi-task maps. In single-task maps, a single map (such as just the height map or the semantic segmentation map) is generated by one pass of the network. In multi-task map, both the height and segmentation maps are generated simultaneously. It is found that the performance of single-task and multi-task approaches are similar, while the multi-task approach provides significant performance boost over single-task map generation.

Secondly, the authors have created a dataset - FB-SSEM dataset. This dataset consists of images from four surround-view fisheye cameras, the motion information of the ego vehicle, ground truth BEV semantic segmentation and height map. The dataset consists of 20 sequences of an ego vehicle in different parking lots, where each sequence consists of 1000 temporal samples. The temporal information is necessary for generating accurate BEVs. This dataset attempts to fill the disparity between the size of datasets between BEV maps and F2BEV maps for public use.

In my opinion, this research helps in solving a very important issue: reducing the number of sensors required to perform autonomous driving. A famous quote by Elon Musk says that we won’t need any other sensors once we solve vision. I think this is true because as humans we only have access to vision sensors (our 2 eyes) to perceive threats around us, and we are rather good drivers. Hence, an autonomous agent, whose sole purpose is to drive, should be able to do so with just surround view cameras. This also reduces the potential points of failure and the processing resources required. 

I also believe that the authors provide a good starting point for further research in generating BEV from wide angle cameras. Better performing and more efficient algorithms can certainly be developed from this point on. The dataset developed (SSEB dataset) should be further expanded to incorporate more scenarios and classes of objects (currently only 5 classes are included).

Open challenges: Baseline, hence computational performance needs to be analyzed. For real time applications, the algorithm should be computationally efficient to allow deployment on-board actual AVs. Thus, a study should be conducted that assesses the latency introduced by the proposed algorithm and what potential improvements can be made to make it more efficient. Further, the authors only consider 5 classes for semantic segmentation and only 3 levels in height map generation. This level of granularity may be insufficient for deployment of the algorithm in complex and rich city environments. Thus, a more comprehensive class set can be explored.


### <a name="topic2"></a>Topic 2: Scaling AV through Adversarial Scenario Generation and Continual Learning.
_Talk by: Yuning Chai (Head of AI Research at Cruise)_

Dr. Chai talks about how generating adversarial scenarios using GANs can help in providing additional learning examples for the agent. He addresses how generative AI can be used to provide a scalable technology that can generate good training data. This is essential to help in expanding the categories of scenarios that are available as training data. 

Modern neural networks are composed of billions of parameters that are very data hungry and need large amounts of data (often in terabytes or even petabytes) to generalize well. The talk  proposes a method of multi-pass learning in which we can eliminate the need of storing old data without sacrificing the performance metrics on old data. This helps in solving the scalability issue in continual learning in AI models.

Generative AI paves the way to generate realistic examples in high-dimensional spaces such as RGB images or 3-dimensional paths. The talk makes an important contribution of using generative AI to generate realistic but improbable pedestrian behavior. This has been accomplished by modifying the architecture of the generator of a traditional GAN where a single neural network is used to produce the output. Here, an encoder and a decoder are used to make up the generator network. An additional embedding is generated from the desired distribution and passed to the decoder that generates an output sample (as seen in fig. 1). By using this method, the authors improve the chances of obtaining ‘valid’ samples that will be more suitable candidates for training the underlying neural network.

<p align="center">
  <img src="images/fig1_cruise.png" alt="Encoder Decoder GAN" width="600" />
  <br/>
  <em>Fig 1: Modified GAN architecture</em>
</p>

Another important contribution in the area of continual learning of neural networks has been made. In most approaches, access to old data (not for retraining, but for evaluation) is often required or the approach is not scalable to multiple fine-tuning stages. In the proposed method, the weight updates are made based on only the new data but using the original checkpoint model’s weights as reference. An example can be seen in fig. 2, where the weight updates are calculated by a combination of gradients obtained from the current model and new data and the difference between the current model and check-pointed model. Using this method, the need to store old data is eliminated while ensuring that the model does not ‘forget’ the old data.


<p align="center">
  <img src="images/fig2_cruise.png" alt="PC grad idea" width="600" />
  <br/>
  <em>Fig. 2: The idea of PC grad</em>
</p>

I believe that the presentation fits well in the context of scalable AI. The authors not only introduce an important issue when dealing with scalable autonomous driving systems, but present interesting approaches towards solving it. The idea of generative AI being used to generate training samples in the context of robotics is gaining traction. It is being used in CNNs, path planning, RL-based control systems, and as already mentioned, in autonomous driving systems. This is important because collecting data for robotic systems is often a laborious task and consists of data from multiple sensors. Thus, collecting real-world data for unlikely scenarios may not be feasible for many applications. If generative AI can help in generating realistic data that is well diversified, the time-to-market can be significantly improved while ensuring that the model can generalize to complicated scenarios. However, a quantitative quality-of-sample should be established for applicability of the proposed method in real-world scenarios.

Further, in the context of continual learning, elimination of dependence on large amounts of data is essential in the longer term, however, the talk does go into a lot of detail regarding the performance benefits of PC grad when compared to other approaches. The idea is enticing and I believe that further analysis should be done to conclude the efficacy of the proposed method.

Though Dr. Chai addresses important issues towards scalable autonomous driving, additional research is necessary to measure and ensure the diversity of samples generated. Generative AI is difficult to train and can lead to inaccurate samples that might misguide the learning process. A metric to measure the ‘validity’ of generated samples should be devised which could help in discarding invalid samples. Such a metric would enable researchers to analyze the distribution of output generated by the AI and further tweak the AI to give more evenly distributed samples that constitute a varied set of scenarios.

### <a name="topic3"></a>Topic 3: A V2V-based Collaborative Navigation Framework for Connected Autonomous Vehicles in Occluded Scenarios.[3](#ref3)

The context of Connected Autonomous Vehicles (CAVs) is an emerging and important topic in the development and deployment of AVs on a large scale. As more and more AVs begin getting deployed, we can leverage the increased number of AVs to form local networks of information sharing between AVs. This is the whole context of CAVs, where two or more AVs are connected through either a central network (such as all vehicles in a city) or through a local network (such as at an intersection). Such an approach allows AVs to not only make decisions based on the data acquired using on-board sensors, but also take advantage of other AVs around the ego vehicle to get more information about the environment. This allows for safer operation and increased road usage efficiency.

The main contribution of [3](#ref3) is in the area of CAVs, where the authors propose an efficient framework for coordinating between multiple AVs and execute a policy to minimize the collision probability and maximize the traffic throughput. This achievement can be viewed in three stages: 
The authors designed a custom Gym environment for Occluded Intersections based on CARLA sim and OpenCDA framework. This was done as there was no available simulator that would facilitate information sharing between vehicles while ensuring high fidelity lidar data representation. The experimentation was done using this custom environment.
To share data efficiently between vehicles, the authors compress the extracted features from the LIDAR data using DRACO 3D compression algorithm. This reduces the network bandwidth requirement by 40 times hence ensuring better performance during the experimental phases.
By using MAPPO (Multi-Agent Proximal Policy Optimization) algorithm, the agents find optimum optimum policy by utilizing all the information available on-board and through shared information. This has reportedly reduced the collision rate by over 70% when compared to rule-based methods. The proposed method also improves average speed of the ego vehicle by 12% which translates to higher traffic throughput.

From my perspective, the fact that there are limited simulation platforms that enable testing of CAVs leads to inferring that one could further develop such coordinated technologies. The authors design a custom gym environment which can be used to train MARL agents in autonomous driving. As per the authors, this was necessary as no existing simulator provided the set of configurations that were necessary for this study. If this is indeed the case, then the field of CAVs has a potential to make important contributions that have a huge impact on industry and academia. Further, limited studies on CAVs, with majority of them being focussed on scheduling schemes through a global controller, imply that further development is needed in this area.

Due to the nacency of the field, I think there are a number of improvements and limitations in the study. Firstly, in the custom environment created, efficacy of the reward function needs to be investigated. Every RL researcher/enthusiast is aware that finding a good reward function is like finding a needle in a haystack. It requires numerous iterations and experimentations. The proposed reward function may be a decent function, but a better reward function may be found that leads to faster convergence and more stable training. 

Another vital issue is that only one sensor, i.e. lidar, is being used. If the sensor fails or its performance metrics dip due to any reason, the entire system may be rendered useless. LiDAR sensors can fail due to multiple reasons such as inclement weather, hardware malfunction, or external blocking of the sensor due to obstacles/dirt in the environment. Hence, some redundant sensors should be fused to make the system more robust.

In CAVs, network latency is an important factor. If there is a significant delay in transporting critical information, this could lead to catastrophic outcomes. While the authors propose data compression, a network latency performance metric should be measured to ensure optimum performance for critical operations.


### <a name="refs"></a>References:
<ol>
  <li>
    <a name="ref1"></a>Samani, Ekta U., Feng Tao, Harshavardhan R. Dasari, Sihao Ding, and Ashis G. Banerjee. "F2BEV: Bird's Eye View Generation from Surround-View Fisheye Camera Images for Automated Driving." arXiv preprint arXiv:2303.03651 (2023).
  </li>
  <li>
    <a name="ref2"></a>Talk 1: Scaling AV Across the US by Dr. Yuning Chai. 
  </li>
  <li>
    <a name="ref3"></a>Leandro Parada, Hanlin Tian, Jose Escribano, and Panagiotis Angeloudis. "A V2V-based Collaborative Navigation Framework for Connected Autonomous Vehicles in Occluded Scenarios.".
  </li>
</ol>

